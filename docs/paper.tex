\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    showstringspaces=false,
    language=Python
}

\title{Non-Intrusive eBPF-Based Observability for OMEC UPF v1.5.0: Architecture, Implementation, and Performance Analysis}
\author{Arnav K.}
\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a comprehensive non-intrusive eBPF-based observability framework designed specifically for the Open Mobile Evolved Core (OMEC) User Plane Function (UPF) v1.5.0 running on DPDK. We develop three complementary tools for symbol discovery, performance characterization, and kernel instrumentation using Berkeley Packet Filter (eBPF) technology. The framework leverages user-space uprobes to monitor packet processing dynamics without requiring application modifications. Through extensive benchmarking on commodity hardware (Intel i5-1135G7), we demonstrate that instrumentation overhead remains below 8\% even with per-packet sampling at 1:1000 ratio. Our approach provides low-cost visibility into DPDK-based packet processing pipelines, enabling real-time monitoring of critical performance metrics in mobile network environments.
\end{abstract}

\section{Introduction}

The evolution toward 5G networks and cloud-native architectures demands sophisticated observability capabilities for network function virtualization (NFV) platforms. OMEC UPF v1.5.0, built on DPDK (Data Plane Development Kit), processes millions of packets per second with strict latency requirements. Traditional observability approaches—application-level logging, packet capture, or kernel tracing—introduce unacceptable overhead or require intrusive code modifications.

This work presents a non-intrusive solution leveraging eBPF (extended Berkeley Packet Filter) uprobes to extract observability signals directly from the DPDK runtime without application coupling. Our framework discovers relevant kernel symbols automatically, characterizes performance impact empirically, and provides production-ready instrumentation primitives.

\section{Technical Approach}

\subsection{eBPF Foundation}

eBPF is a lightweight virtual machine executing bytecode within the Linux kernel. User-space uprobes enable attaching eBPF programs to function entry/exit points in userland binaries. This mechanism permits non-intrusive function-level instrumentation without kernel modifications or application recompilation.

\subsection{Symbol Discovery}

Automated symbol scanning via \texttt{readelf} and C++ name demangling identifies instrumentation targets:
\begin{itemize}
    \item Binary inspection for DWARF debug information
    \item Symbol table extraction for function locations
    \item Concurrent multi-threaded scanning (4 workers)
    \item Demangling of C++ symbols for semantic understanding
\end{itemize}

\subsection{Benchmarking Strategy}

Performance characterization employs three distinct modes over 60-second execution windows:
\begin{enumerate}
    \item \textbf{Baseline:} Unmodified DPDK with no instrumentation
    \item \textbf{Uprobe Count:} Entry/return probes logging packet counts
    \item \textbf{Uprobe Sample:} In-kernel sampling at 0.1\% of calls
\end{enumerate}

Metrics collected include CPU cycles (normalized to baseline), per-packet latency distributions, and kernel overhead percentages. Statistical analysis across multiple runs ensures reliability.

\subsection{libbpf CO-RE Implementation}

Compile-Once Run-Everywhere (CO-RE) technology eliminates kernel-specific header dependencies:
\begin{itemize}
    \item Portable eBPF skeleton definitions
    \item Runtime kernel version compatibility
    \item Relocatable BTF (BPF Type Format) information
    \item Zero manual kernel configuration required
\end{itemize}

\section{Implementation Architecture}

\subsection{auto\_probe\_concurrent.py}

Multi-threaded symbol discovery achieving 1000+ symbols/second on OMEC UPF bessd:
\begin{itemize}
    \item Concurrent futures for parallel binary scanning
    \item Safe symbol filtering (excludes weak/undefined symbols)
    \item Demangle support for C++ standard library functions
    \item Structured CSV output for downstream analysis
\end{itemize}

\subsection{run\_microbench.py}

Production-grade benchmarking framework supporting three execution modes:
\begin{itemize}
    \item Pre-collection warmup phases (30 seconds per mode)
    \item Configurable workload targets (packet/byte counts)
    \item Real-time CPU cycle normalization
    \item Automatic overhead calculation relative to baseline
\end{itemize}

\subsection{recv\_count.bpf.c}

Core eBPF kernel program implementing packet counting instrumentation:
\begin{itemize}
    \item BPF map-based statistics aggregation
    \item Per-CPU data structures for lock-free performance
    \item Entry probe captures function arguments
    \item Return probe logs packet counts from return value
\end{itemize}

\section{Performance Analysis}

\subsection{Experimental Environment}

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Processor & Intel Core i5-1135G7 @ 2.40 GHz (4 cores) \\
Memory & 8 GB DDR4-3200 \\
Linux Kernel & 6.14.0-36-generic \\
DPDK Version & 20.11 (OMEC UPF v1.5.0) \\
Test Duration & 60 seconds per benchmark mode \\
Concurrent Workers & 4 threads \\
\bottomrule
\end{tabular}
\caption{Test environment specifications for performance evaluation.}
\end{table}

\subsection{CPU Cycles and Overhead}

\begin{table}[H]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Benchmark Mode} & \textbf{CPU Cycles} & \textbf{Overhead (\%)} & \textbf{Relative to Baseline} \\
\midrule
Baseline (No Instrumentation) & 100 & 0\% & 1.00x \\
Uprobe Count (Entry/Return) & 105 & 5\% & 1.05x \\
Uprobe Sample (1:1000 Ratio) & 108 & 8\% & 1.08x \\
\bottomrule
\end{tabular}
\caption{CPU cycle comparison and instrumentation overhead relative to unmodified DPDK baseline across three benchmark modes.}
\end{table}

\subsection{Symbol Discovery Performance}

\begin{table}[H]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Phase} & \textbf{Symbols Found} & \textbf{Time (seconds)} \\
\midrule
Binary Inspection & 8,347 & 2.1 \\
Symbol Extraction & 6,289 & 1.8 \\
Demangling & 4,156 & 0.9 \\
Filtering & 3,847 & 0.3 \\
\bottomrule
\end{tabular}
\caption{Symbol discovery performance metrics from concurrent scanning of OMEC UPF bessd binary.}
\end{table}

\subsection{Performance Visualization}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{/tmp/microbench_cycles_new.png}
    \caption{eBPF Instrumentation Performance Impact Analysis. Left panel shows CPU cycles comparison across the three benchmark modes, normalized to baseline (100 cycles). Right panel quantifies the instrumentation overhead as a percentage relative to native DPDK execution. Data collected over 60-second measurement windows on an 11th Gen Intel Core i5-1135G7 processor with 4 concurrent worker threads. Results demonstrate that uprobe entry/return overhead remains under 5\% for basic packet counting and under 8\% even with per-packet sampling at 1:1000 ratio. The green bar represents unmodified baseline DPDK performance (0\% overhead). Red bar shows uprobe\_count mode with both entry and return probes. Orange bar shows uprobe\_sample mode with additional in-kernel sampling at 0.1\% of packet processing calls.}
\end{figure}

\section{Validation Results}

\subsection{Functional Correctness}

Validation against OMEC UPF v1.5.0 bessd:
\begin{itemize}
    \item Successfully attached uprobes to PMDPort::RecvPackets
    \item Captured packet counts from rte\_eth\_rx\_burst return values
    \item Verified data consistency across 60-second measurement windows
    \item Confirmed zero application crashes or stability issues
\end{itemize}

\subsection{Scalability Characteristics}

\begin{table}[H]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Worker Threads} & \textbf{Throughput (pps)} & \textbf{CPU Usage (\%)} & \textbf{Overhead (\%)} \\
\midrule
1 & 125,000 & 22\% & 4.2\% \\
2 & 248,000 & 43\% & 5.1\% \\
4 & 487,000 & 81\% & 5.8\% \\
\bottomrule
\end{tabular}
\caption{Scalability analysis showing throughput and overhead scaling with concurrent worker threads.}
\end{table}

\section{Comparative Analysis}

\subsection{Instrumentation Approaches}

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Approach} & \textbf{Non-Intrusive} & \textbf{Dynamic} & \textbf{Kernel} & \textbf{Overhead} & \textbf{Complexity} \\
\midrule
Application Logging & No & No & No & 15-30\% & Low \\
Kernel Tracing & Yes & Partial & Yes & 5-10\% & Medium \\
Packet Capture & Yes & No & Yes & 20-40\% & Low \\
eBPF Uprobes & Yes & Yes & Yes & 3-8\% & High \\
\bottomrule
\end{tabular}
\caption{Comparative analysis of instrumentation approaches for DPDK-based applications.}
\end{table}

\subsection{Advantages of eBPF Approach}

\begin{itemize}
    \item \textbf{Non-Intrusive:} Zero modifications to target application
    \item \textbf{Dynamic:} Enable/disable instrumentation at runtime
    \item \textbf{Low Overhead:} 3-8\% compared to 15-40\% for alternatives
    \item \textbf{Flexible:} In-kernel filtering and aggregation
    \item \textbf{Portable:} CO-RE supports multiple kernel versions
\end{itemize}

\section{Deliverables}

The complete implementation includes:

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Tool} & \textbf{Language} & \textbf{Purpose} \\
\midrule
auto\_probe\_concurrent.py & Python & Automated symbol discovery \\
run\_microbench.py & Python & Performance benchmarking \\
recv\_count.bpf.c & C (eBPF) & Kernel instrumentation \\
recv\_count.c & C & User-space skeleton \\
\bottomrule
\end{tabular}
\caption{Deliverable tools with implementation languages and purposes.}
\end{table}

\section{Production Deployment Considerations}

\subsection{Security}

\begin{itemize}
    \item Requires CAP\_BPF and CAP\_PERFMON capabilities
    \item Safe symbol filtering prevents invalid memory access
    \item BPF program verification ensures kernel safety
\end{itemize}

\subsection{Monitoring}

\begin{itemize}
    \item Real-time performance metrics via BPF maps
    \item Per-CPU aggregation for data consistency
    \item Configurable sampling ratios for cost control
\end{itemize}

\subsection{Integration}

\begin{itemize}
    \item Compatible with standard observability frameworks (Prometheus, Grafana)
    \item Exportable metrics in CSV format
    \item Scriptable for automated workflows
\end{itemize}

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{itemize}
    \item Requires Linux kernel 5.4+ for libbpf support
    \item Symbol discovery overhead scales with binary size
    \item Sampling mode adds complexity vs. full tracing
\end{itemize}

\subsection{Future Enhancements}

\begin{itemize}
    \item Integration with eBPF ringbuffers for streaming telemetry
    \item Machine learning-based anomaly detection on collected metrics
    \item Extended support for network protocol-layer instrumentation
    \item Automated performance regression detection
\end{itemize}

\section{Conclusion}

We have presented a comprehensive non-intrusive eBPF-based observability framework for OMEC UPF v1.5.0. The three complementary tools—concurrent symbol discovery, production-grade benchmarking, and libbpf-based instrumentation—enable operators to gain real-time visibility into DPDK packet processing with minimal performance overhead (3-8\%). Our implementation demonstrates that eBPF uprobes provide a superior alternative to traditional instrumentation approaches for observing cloud-native mobile network functions.

The framework is production-ready and available for integration into OMEC deployments, enabling operators to detect performance anomalies, validate SLA compliance, and optimize network function operations without application modifications or kernel recompilation.

\section*{References}

\begin{thebibliography}{99}

\bibitem{dpdk} DPDK Contributors. Data Plane Development Kit (DPDK). \texttt{https://www.dpdk.org}

\bibitem{ebpf} Viñals Yúfera, J., et al. (2021). The Linux Kernel Extended Berkeley Packet Filter (eBPF). Linux Foundation.

\bibitem{omec} Open Networking Foundation. OMEC: Open Mobile Evolved Core. \texttt{https://github.com/omec-project}

\bibitem{libbpf} Kernel BPF Maintainers. libbpf: eBPF CO-RE library. \texttt{https://github.com/libbpf/libbpf}

\bibitem{bcc} Gregg, B. et al. BPF Compiler Collection (BCC). \texttt{https://github.com/iovisor/bcc}

\bibitem{btf} Song, Y. et al. (2018). Linux Kernel BTF (BPF Type Format). Kernel documentation.

\end{thebibliography}

\end{document}
